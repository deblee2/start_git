# -*- coding: utf-8 -*-
"""1669063_esg_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B-v0FfcP5dVT0hIKAkoGJEkgD9eE1dZh

## 프로젝트 주제: ESG 뉴스 분석
*네이버 뉴스에서 ESG 뉴스 데이터 수집, csv 파일 생성 및 활용, 형태소 분석 기반 worldcloud 그리기*

**제안 배경**

```
# 세계 최대 자산운용사 블랙락은 "지속가능성을 투자기준으로 삼겠다"라고 말했습니다. 
이처럼 ESG(환경, 사회, 지배구조)는 기업을 평가하는 비재무적인 지표로 떠오르고 있으며, ESG 기반 투자 자산은 빠르게 증가하고 있습니다. 
그리하여 ESG에 대하여 프로젝트를 진행해보면서 관련 토픽에 대하여 알아보고, 추후 기말 프로젝트에서 텍스트마이닝 기반 ESG 요소 평가 시스템을 만들어보려고 합니다.
```

**구현 프로젝트**
```
# 네이버뉴스에서 ESG 관련 뉴스를 스크래핑해서 언론사, 제목, 기사내용, url을 csv 파일로 만들었습니다. 
그 다음, csv 파일을 업로드하여 뉴스 제목과 기사 내용 일부를 가지고 각각 Kkma와 Twitter를 사용하여 분석하고, 워드클라우드를 생성하였습니다.
뉴스 제목으로 워드클라우드를 만들 때는 나뭇잎 이미지를 가져와서 실루엣처럼 활용하여 만들어보았습니다.
```
 

**프로젝트 진행 방향**
```
# 앞으로 기말 프로젝트는 현 프로젝트에서 확장하여 진행하려고 합니다. 
E, S, G 관련 용어 딕셔너리를 구축한 뒤, 용어가 뉴스에 나올 경우, 문장의 긍/부정을 분석하여 ESG에 반영하려고 합니다.
즉, 뉴스기사 AI 분석을 통하여 ESG 요소를 파악하는 서비스를 구축해보려고 합니다. 
이를 통하여 실제로 활용가능한 프로그램을 만들어보는 것이 이번 학기의 목표입니다.
```

## 목차
### 1. ESG 뉴스 스크래핑하기
### 2. csv 파일 열어서 데이터 분석하고 wordcloud 그리기
#### (1) ESG 제목 데이터로 wordcloud 그리기
#### (2) ESG 기사 내용 데이터로 wordcloud 그리기
```
(2)-1 ESG 기사 내용 [명사] wordcloud 그리기
(2)-2 ESG 기사 내용 [형용사] wordcloud 그리기
(2)-3 ESG 기사 내용 [동사] wordcloud 그리기
```
"""

!pip install konlpy

!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

# 패키지 불러오기
import numpy as np                    
import pandas as pd                   
import matplotlib.pyplot as plt       

from collections import Counter

from bs4 import BeautifulSoup
import requests

import warnings
warnings.filterwarnings('ignore')

"""## 1. ESG 뉴스 스크래핑하기"""

# 해드라인 검색 및 추출 함수를 정의
t_list = []

def web_scraping(keyword, end, start = 1): 
    # 네이버 뉴스 url의 특성을 고려하여 추가된 수식
    end = (end-1)*10 + 1
        
    while 1:
        
        if start > end :    # 스타트 페이지가 마지막 페이지보다 크면 while 문을 빠져 나감 
            break        
        
        url ='https://search.naver.com/search.naver?where=news&sm=tab_pge&query={0}&start={1}'.format(keyword,start)

        req = requests.get(url)    # 해당 페이지를 가져옴

        page = BeautifulSoup(req.text, 'html.parser')
        
        news_titles = page.find_all(attrs='news_tit')
        medias = page.find_all(attrs='info press') 
        texts = page.find_all('a', attrs='api_txt_lines dsc_txt_wrap')

        # 검색 결과 중에서 큰 제목 & 해당 사이트 주소 추출 
        ablist=[] 
        for one in news_titles :
            a = one.attrs['title']
            b = one.attrs['href']
            ablist.append([a, b])
        
        # 언론사 추출
        i = 0
        for one in medias :
            c = one.text
            ablist[i].insert(0, c)
            i += 1

        # 기사 본문 내용 일부 추출
        j = 0
        for one in texts :
            d = one.text
            ablist[j].insert(2, d)
            j += 1
        
        t_list.append(ablist)
        
        start += 10

# 검색어를 ESG로 설정 

#keyword = input('검색어를 입력하세요 : ')
keyword = 'ESG'
print()

# 마지막 페이지를 1000으로 설정
#end_page = int(input('마지막 페이지 번호를 입력하세요 : '))
end_page = 1000
web_scraping(keyword, end_page)       # end_page 페이지 까지 스크래핑

# 총 1000 페이지 스크래핑함
len(t_list)

# 한 페이지 당 10개의 기사가 있음. 이를 t_list 안에 리스트로 넣어놓음
t_list[0]

# 리스트 안 리스트를 꺼내서 각 페이지별로 리스트에 있는 것이 아니라 
#하나의 리스트 안에 모든 페이지 정보가 들어가도록 구성함 
t_list_final=[]

for k in t_list:
    for m in k:
        t_list_final.append(m)
print(len(t_list_final))

# csv 파일로 만들었음 ['media', 'title', 'text', 'url']

df = pd.DataFrame(t_list_final)
df.columns = ['media', 'title', 'text', 'url']
df.head()

# 데이터프레임을 csv로 변환하여 저장함
df.to_csv('C:\\Users\\user\\Documents\\web_text_mining_ewha\\esg_news.csv')

"""## 2. csv 파일 열어서 데이터 분석하고 wordcloud 그리기
### (1) ESG 제목 데이터로 wordcloud 그리기
"""

# esg_news 데이터 업로드하기
from google.colab import files
file_uploaded = files.upload()

# csv 파일 dataframe으로 열기
import io
import pandas as pd

df2 = pd.read_csv(io.BytesIO(file_uploaded['esg_news.csv']))

df2.head()

# 데이터 분석을 위하여 데이터프레임에서 제목만 추출함
title_li = df2['title'].to_list()
title_li

# Kkma로 형태소 분석
from konlpy.tag import Kkma

kkma = Kkma()

sentences_tag = []

for sentence in title_li:
    word_tag = kkma.pos(sentence)
    sentences_tag.append(word_tag)

print(sentences_tag)

# 형태소 분석 후 명사만 추출

noun_list = []

for sentence in sentences_tag:
    for word, tag in sentence: #word와 tag 중에서 word 단어만
        if tag in ['NNG']:               
            noun_list.append(word)
            
print(noun_list)

# 명사 중에서 두음절 단어만 추출 

print('전체 명사의 수: ', len(noun_list))
print() 

noun_list = [word for word in noun_list if len(word) > 1]    # 명사중에서 두음절 이상의 단어  추출

print('두음절 이상의 명사의 수: ', len(noun_list))
print() 

print(noun_list[:100])   # 처음부터 나오는 순서대로 100개 단어 출력

# 단어의 출현 횟수 카운트

counts = Counter(noun_list)
words = counts.most_common(50)     # 가장 많이 출현한 횟수 순으로 50개 단어만 추출
print(words)

# 실루엣을 이용한 워드 클라우드를 위해 이미지 업로드
from google.colab import files
file_uploaded = files.upload()

# 워드 클라우드

import numpy as np                    
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from wordcloud import WordCloud
from wordcloud  import ImageColorGenerator

# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처
mpl.rcParams['axes.unicode_minus'] = False

# 실루엣 이미지 업로드
cloud_img = plt.imread('leaf.jpg')

# 워드 클라우드 세팅
wordcloud = WordCloud(font_path='NanumBarunGothic',
                      background_color='white',
                      random_state = 1,
                      color_func = ImageColorGenerator(cloud_img),   # img의 color 를 이용
                      mask = cloud_img)      # 그림 이미지에 맞게 그림 

print(dict(words))

# 워드클라우드 이미지 print
cloud = wordcloud.generate_from_frequencies(dict(words))  

fig = plt.figure(figsize=(12, 12))
plt.axis('off')
plt.imshow(cloud)
plt.show()

"""### (2) ESG 기사 내용 데이터로 wordcloud 그리기
#### (2)-1 ESG 기사 내용 [명사] wordcloud 그리기
"""

# 데이터 분석을 위하여 데이터프레임에서 '기사 내용 일부'만 추출함
title_li2 = df2['text'].to_list()
#title_li2

# '기사 내용'의 형태소 분석을 하기 위해 리스트 변형
'''
'제목' 분석 시에는 형태소 분석 전에 별다른 변형/조작이 필요 없었는데, 
각 요소가 2개 이상의 문장으로 이루어진 경우, 형태소 분석기에서 인식을 하지 못하고 TypeError 오류가 남.
그래서 '기사 내용' 분석을 위해서는 문장 별로 잘라준 뒤에 형태소 분석을 해야됨.
'''

real_li = []
for j in range(10000):
  a=str(title_li2[j]).split('.')
  #print(a)
  sample_list = list(filter(None, a))
  real_li.append(sample_list)

whole_li = []
for i in real_li:
  for j in i:
    whole_li.append(j)

print(whole_li[:5])

# Kkma가 시간이 오래걸려서 '제목'보다 데이터 크기가 더 큰 '기사 내용 일부'를 분석할 때는 Twitter를 사용해봄 
# 형태소 분석

from konlpy.tag import Twitter
 
twitter = Twitter()
sentences_tag2 = []

for sentence2 in whole_li:
    word_tag2 = twitter.pos(sentence2)
    sentences_tag2.append(word_tag2)

sentences_tag2

# 형태소 분석 후 명사만 추출
noun_list2 = []

for sentence2 in sentences_tag2:
    for word2, tag2 in sentence2:
        if tag2 in ['Noun']:               
            noun_list2.append(word2)

print('명사 추출')            
print(noun_list2)

# 명사 중에서 두음절 단어만 추출 
print('전체 명사의 수: ', len(noun_list2))
print() 

noun_list2 = [word for word in noun_list2 if len(word) > 1]    # 명사중에서 두음절 이상의 단어  추출

print('두음절 이상의 명사의 수: ', len(noun_list2))
print() 

print('처음 나오는 순서대로 100개 출력: ')
print(noun_list2[:100])   # 처음부터 나오는 순서대로 100개 단어 출력

# 단어의 출현 횟수 카운트

counts2 = Counter(noun_list2)
words2 = counts2.most_common(50)     # 가장 많이 출현한 횟수 순으로 50개 단어만 추출
print('가장 많이 출현한 횟수 순으로 50개 추출: ')
print(words2)

# 워드 클라우드 (colormap style을 BrBG로 변경)
from wordcloud import WordCloud
import matplotlib as mpl
import matplotlib.pyplot as plt

#폰트 깨짐 방지
mpl.rcParams['axes.unicode_minus'] = False

wordcloud = WordCloud(font_path='NanumBarunGothic',
                      colormap = 'BrBG',
                      width=600,
                      height=600)

print('딕셔너리 형태 출력')
print(dict(words2))

# 워드 클라우드 print
'''
'제목' 분석 시 이미지를 불러와 실루엣 방법을 사용함.
'기사 내용' 분석 시에는 일반적인 워드 클라우드를 사용하고, colormap 색을 ESG를 표현할 수 있는 색으로 선택함.
'''

cloud = wordcloud.generate_from_frequencies(dict(words2))
plt.rcParams['font.family'] = 'NanumBarunGothic'

plt.figure(figsize=(10, 10))
plt.axis('off')   
plt.imshow(cloud)
plt.show()

"""# (추가 실험) 명사만이 아니라 형용사와 동사 분석도 유의미할까?

## (2)-2 ESG 기사 내용 [형용사] wordcloud 그리기
### 세 음절 이상 추출, 불용어 필터링
"""

# 불용어 작업을 위하여 미리 업로드해놓음
!pip install nltk

import nltk
nltk.download('punkt')

# 형태소 분석 후 형용사만 추출

adj_list = []

for sentence3 in sentences_tag2:
    for word3, tag3 in sentence3: 
        if tag3 in ['Adjective']:               
            adj_list.append(word3)

print('형용사 추출: ')        
print(adj_list)

# 형용사 중에서 세 음절 이상 단어만 추출
print('전체 형용사의 수: ', len(adj_list))
print() 

adj_list = [word for word in adj_list if len(word) > 2]    # 명사중에서 세 음절 이상의 단어  추출

print('세음절 이상의 형용사의 수: ', len(adj_list))
print() 

print('처음 나오는 순서대로 100개 출력: ')
print(adj_list[:100])   # 처음부터 나오는 순서대로 100개 단어 출력

# 불용어 load하기
# 직접 만든 stopword dictionary file 업로드
from google.colab import files
file_uploaded = files.upload()

f = open("kor_stopwords.txt", 'r')
data = f.read()
#data = str(data)
print('불용어 사전 용어: ')
print(data)

# 불용어 제거하기
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 

stop_words=data.split(' ') 

result = [] 

for w in adj_list: 
	if w not in stop_words: 
		result.append(w) 

print('불용어 제거한 형용사 리스트: ')
print(result)

# 단어의 출현 횟수 카운트

counts3 = Counter(result)
words3 = counts3.most_common(50)     # 가장 많이 출현한 횟수 순으로 50개 단어만 추출

# 워드 클라우드 (colormap style을 변경)
from wordcloud import WordCloud
import matplotlib as mpl
import matplotlib.pyplot as plt

#폰트 깨짐 방지
mpl.rcParams['axes.unicode_minus'] = False

wordcloud = WordCloud(font_path='NanumBarunGothic',
                      colormap = 'BrBG',
                      width=600,
                      height=600)

print()
print('딕셔너리 형태 출력')
print(dict(words3))

# 워드 클라우드 print
cloud = wordcloud.generate_from_frequencies(dict(words3))
plt.rcParams['font.family'] = 'NanumBarunGothic'

plt.figure(figsize=(10, 10))
plt.axis('off')   
plt.imshow(cloud)
plt.show()

"""## (2)-3 ESG 기사 내용 [동사] wordcloud 그리기
### 세 음절 이상 추출, stopwords 필터링 수행
"""

# 형태소 분석 후 동사만 추출

verb_list = []

for sentence3 in sentences_tag2:
    for word3, tag3 in sentence3:
        if tag3 in ['Verb']:               
            verb_list.append(word3)

print('동사 추출')           
print(verb_list)

# 동사 중에서 세음절 이상 단어만 추출 

print('전체 동사의 수: ', len(verb_list))
print() 

verb_list = [word for word in verb_list if len(word) > 2]    # 명사중에서 세음절 이상의 단어  추출

print('세음절 이상의 동사의 수: ', len(verb_list))
print()

# 불용어 제거하기
result = []

for w in verb_list: 
	if w not in stop_words: 
		result.append(w) 

print('불용어 제거한 형용사 리스트: ')
print(result)

# 단어의 출현 횟수 카운트
counts3 = Counter(result)
words3 = counts3.most_common(50)     # 가장 많이 출현한 횟수 순으로 50개 단어만 추출
#print(words3)

# 워드 클라우드 (colormap style을 변경)
from wordcloud import WordCloud
import matplotlib as mpl
import matplotlib.pyplot as plt

#폰트 깨짐 방지
mpl.rcParams['axes.unicode_minus'] = False

wordcloud = WordCloud(font_path='NanumBarunGothic',
                      colormap = 'BrBG',
                      width=600,
                      height=600)
print('')
print('딕셔너리 형태 출력')
print(dict(words3))
print()

# 워드 클라우드 print
cloud = wordcloud.generate_from_frequencies(dict(words3))
plt.rcParams['font.family'] = 'NanumBarunGothic'

plt.figure(figsize=(10, 10))
plt.axis('off')   
plt.imshow(cloud)
plt.show()

"""## 추가 실험 결과
* 형용사와 동사에 대해서 세 음절 이상 단어와 불용어 사전으로 필터링을 했음
* 하지만 화한다, 하고자, 했다고 등이 주로 나온 것으로 보아 동사 분석에 비해서 유의미하다고 볼 수는 없음
* 불용어 사전의 데이터를 더 모으고, stemmer/lemmatization 등을 해본다면 더 유용한 정보를 얻을 수 있을 것이라고 기대됨
"""

