{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "double-sculpture",
   "metadata": {},
   "source": [
    "# 04. 카운트 기반의 단어 표현\n",
    "## 2) Bag of Words(BoW)\n",
    "* 단어의 등장 순서를 고려하지 않는 빈도수 기반의 단어 표현 방법\n",
    "\n",
    "\n",
    "* (1) 각 단어에 고유한 정수 인덱스 부여\n",
    "* (2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romance-judge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt=Okt()\n",
    "\n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업\n",
    "token=re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\") \n",
    "token=okt.morphs(token) #OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤, token에다가 넣음\n",
    "\n",
    "word2index = {}\n",
    "bow=[]\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index) # token읽으면서 word2index에 없는 단어는 새로 추가\n",
    "        bow.insert(len(word2index)-1, 1) # bow 전체에 전부 기본값 1을 넣어줌. 단어의 개수는 최소 1이상이기 때문\n",
    "    else:\n",
    "        index=word2index.get(voca) #재등장하는 단어의 인덱스를 받음\n",
    "        bow[index]=bow[index]+1 #재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줌(단어의 개수를 세는 것)\n",
    "print(word2index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tough-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ongoing-dependence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt=Okt()\n",
    "\n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업\n",
    "token=re.sub(\"(\\.)\",\"\",\"소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\") \n",
    "token=okt.morphs(token) #OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤, token에다가 넣음\n",
    "\n",
    "word2index = {}\n",
    "bow=[]\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index) # token읽으면서 word2index에 없는 단어는 새로 추가\n",
    "        bow.insert(len(word2index)-1, 1) # bow 전체에 전부 기본값 1을 넣어줌. 단어의 개수는 최소 1이상이기 때문\n",
    "    else:\n",
    "        index=word2index.get(voca) #재등장하는 단어의 인덱스를 받음\n",
    "        bow[index]=bow[index]+1 #재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줌(단어의 개수를 세는 것)\n",
    "print(word2index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "promotional-venue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "departmental-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "okt=Okt()\n",
    "\n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업\n",
    "token=re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\") \n",
    "token=okt.morphs(token) #OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤, token에다가 넣음\n",
    "\n",
    "word2index = {}\n",
    "bow=[]\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index) # token읽으면서 word2index에 없는 단어는 새로 추가\n",
    "        bow.insert(len(word2index)-1, 1) # bow 전체에 전부 기본값 1을 넣어줌. 단어의 개수는 최소 1이상이기 때문\n",
    "    else:\n",
    "        index=word2index.get(voca) #재등장하는 단어의 인덱스를 받음\n",
    "        bow[index]=bow[index]+1 #재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줌(단어의 개수를 세는 것)\n",
    "print(word2index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "obvious-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-people",
   "metadata": {},
   "source": [
    "## 3. CountVectorizer 클래스로 BoW 만들기\n",
    "* 사이킷런에서는 단어의 빈도를 count하여 vector로 만드는 CountVectorizer 클래스를 지원\n",
    "* 이를 이용하면 영어에 대해서는 BoW 쉽게 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hispanic-improvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) #코퍼스로부터 각 단어의 빈도 수를 기록\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-wildlife",
   "metadata": {},
   "source": [
    "## 4. 불용어를 제거한 BoW 만들기\n",
    "* 영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "future-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "# (1) 사용자가 직접 정의한 불용어 사용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text= [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=['the', 'a', 'an', 'is', 'not'])\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interim-there",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "# (2) CountVectorizer에서 제공하는 자체 불용어 사용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text=[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "right-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "# (3) NLTK에서 지원하는 불용어 사용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words = sw)\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-saturday",
   "metadata": {},
   "source": [
    "## 3) 문서 단어 행렬(Document-Term Matrix, DTM)\n",
    "### 1. 문서 단어 행렬(DTM)의 표기법\n",
    "* 다수의 문서에 등장하는 각 단어들의 빈도를 행렬로 표현한 것\n",
    "* 각 문서에 대한 BoW를 하나의 행렬로 만든 것. BoW와 다른 표현 방법이 아니라, **BoW 표현을 다수의 문서에 대해서 행렬로 표현하고 부르는 용어**\n",
    "* 각 문서에서 등장한 단어의 빈도를 행렬의 값으로 표기\n",
    "\n",
    "\n",
    "* 문서 단어 행렬은 문서들을 서로 비교할 수 있도록 수치화할 수 있다는 점에서 의의를 가짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-retrieval",
   "metadata": {},
   "source": [
    "### 2. 문서 단어 행렬(Document-Term Matrix)의 한계\n",
    "#### 1) 희소 표현(Sparse representation)\n",
    "* 원-핫 벡터는 단어 집합의 크기가 벡터의 차원이 되고 대부분 값이 0이 된다는 특징이 있었음. \n",
    "* 이 특징은 공간적 낭비와 계산 리소스를 증가시킬 수 있다는 점에서 원-핫 벡터의 단점\n",
    "* DTM도 마찬가지. DTM에서 각 행을 문서 벡터라고 하면 그것의 각 차원은 원-핫 벡터와 마찬가지로 전체 단어 집합의 크기를 가짐\n",
    "* 만약 전체 코퍼스가 방대한 데이터라면 문서 벡터의 차원은 수백만의 차원을 가질 수도 있음\n",
    "\n",
    "* 대부분의 값이 0이 표현: 희소 벡터(sparse vector) 또는 희소 행렬(sparse matrix)\n",
    "    \n",
    "    - 전처리 통한 단어 집합 크기 줄이는 일 매우 중요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-annotation",
   "metadata": {},
   "source": [
    "#### 2) 단순 빈도 수 기반 접근\n",
    "* 불용어 빈도수 높음\n",
    "* 불용어와 중요한 단어에 대해서 가중치 주기 --> TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-purchase",
   "metadata": {},
   "source": [
    "## 3. TD-IDF(Term Frequency-Inverse Document Frequency)\n",
    "### 1. TF-IDF(단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency)\n",
    "* 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법\n",
    "* DTM을 만든 후, TF-IDF 가중치를 부여\n",
    "* 문서의 유사도, 검색 시스템에서 검색 결과의 중요도 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업\n",
    "* **TF-IDF = TF * IDF**\n",
    "    * 문서: d, 단어: t, 문서 총 개수: n\n",
    "    * (1) **tf(d, t)**: 특정 문서 d에서의 특정 단어 t의 등장 횟수\n",
    "        - TF: DTM의 각 단어들이 가진 값\n",
    "    * (2) **df(t)**: 특정 단어 t가 등장한 문서의 수\n",
    "        - ex) 바나나가 문서 2와 3에 등장-> df=2\n",
    "    * (3) **idf(d, t)**: df(t)에 반비례하는 수\n",
    "        - idf(d, t) = log(n/1+df(t))\n",
    "        - 역수만 취해주면 기하급수적으로 커질 수 있어 log와 분모에 1을 더해줌\n",
    "        \n",
    "        \n",
    "* 모든 문서에서 자주 등장하는 단어는 중요도가 낮다\n",
    "* 특정 문서에서만 자주 등장하는 단어는 중요도가 높다\n",
    "* TF-IDF 값이 높을수록 중요도가 큼\n",
    "* IDF는 여러 문서에서 등장한 단어의 가중치를 낮추는 역할\n",
    "\n",
    "\n",
    "* 문서 2에서는 바나나를 한 번 언급했지만, 문서3에서는 바바나를 두 번 언급했기 때문에 문서 3에서의 바나나를 더욱 중요한 단어라고 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-allocation",
   "metadata": {},
   "source": [
    "### 2. 파이썬으로 TF-IDF 직접 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 데이터프레임 사용을 위해\n",
    "from math import log \n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF, IDF, TF-IDF 값을 구하는 함수\n",
    "N = len(docs) # 총 문서의 수\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df + 1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "logical-academy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF\n",
    "result = []\n",
    "for i in range(N): # 각 문서에 대해서 아래 명령을 수행\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]        \n",
    "        result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "expected-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IDF\n",
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index = vocab, columns = [\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "coral-parameter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 행렬\n",
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]\n",
    "        result[-1].append(tfidf(t,d))\n",
    "tfidf_ = pd.DataFrame(result, columns=vocab)\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-responsibility",
   "metadata": {},
   "source": [
    "### 3. 사이킷런을 이용한 DTM과 TF-IDF 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "vital-january",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do',\n",
    "]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도수를 기록함\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-latest",
   "metadata": {},
   "source": [
    "* 사잇킷런의 TfidfVectorizer\n",
    "    * IDF의 로그항의 분자에 1을 더해주며, 로그항에 1을 더해주고, TF-IDF에 L2 정규화라는 방법으로 값을 조정하는 등의 차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cardiovascular-raising",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do',\n",
    "]\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-composite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
