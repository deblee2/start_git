{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cubic-bibliography",
   "metadata": {},
   "source": [
    "## 1) 언어 모델이란?\n",
    "* 언어라는 현상을 모델링하고자 단어 시퀀스 또는 문장에 확률을 할당하는 모델\n",
    "* 크게 통계를 이용한 방법과 인공 신경망을 이용한 방법으로 구분할 수 있음\n",
    "\n",
    "### 1. 언어 모델\n",
    "* 언어 모델은 **단어 시퀀스에 확률을 할당하는 일**을 하는 모델\n",
    "* a.k.a. 가장 자연스러운 단어 시퀀스를 찾아내는 모델\n",
    "* 단어 시퀀스에 확률을 할당하기 위해서 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것\n",
    "    혹은 양쪽 단어들로부터 가운데 비어있는 단어를 예측하는 언어모델(BERT 챕터에서 다룰것)\n",
    "\n",
    "\n",
    "* **언어 모델링(Language Modeling)**: 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업\n",
    "    즉, 언어 모델이 이전 단어들로부터 다음 단어를 예측하는 일은 언어 모델링임\n",
    "* 스탠포드에서는 언어 모델을 문법(grammar)에 비유하기도 함. 언어 모델이 단어들의 조합이 얼마나 적절한지, 또는 해당 문장이 얼마나 적합한지를 알려주는 일을 하는 것이 마치 문법이 하는 일 같기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-connecticut",
   "metadata": {},
   "source": [
    "### 2. 단어 시퀀스의 확률 할당\n",
    "* 기계 번역, 오타 교정, 음성 인식\n",
    "* 언어 모델은 확률을 통해 보다 적절한 문장을 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-orleans",
   "metadata": {},
   "source": [
    "### 3. 주어진 이전 단어들로부터 다음 단어 예측하기\n",
    "* 언어 모델은 단어 시퀀스에 확률을 할당하는 모델. 그리고 단어 시퀀스에 확률을 할당하기 위해서 가장 보편적으로 사용하는 방법은 이전 단어들이 주어졌을 때, 다음 단어를 예측하도록 하는 것.\n",
    "* 이를 조건부 확률로 표현\n",
    "\n",
    "\n",
    "* 단어 시퀀스의 확률\n",
    "    P(W)=P(w1,w2,w3,w4,w5,...,wn)\n",
    "* 다음 단어 등장 확률\n",
    "    P(wn|w1,...,wn−1)\n",
    "* 전체 단어 시퀀스 W의 확률\n",
    "    P(W)=P(w1,w2,w3,w4,w5,...wn)=∏i=1nP(wn|w1,...,wn−1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-genre",
   "metadata": {},
   "source": [
    "## 2) 통계적 언어 모델 (Statistical Langugae Model, SLM)\n",
    "* 언어 모델의 전통적인 접근 방법\n",
    "\n",
    "* 1. 조건부 확률\n",
    "    조건부 확률의 연쇄 법칙(chain rule): P(x1,x2,x3...xn)=P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1...xn−1)\n",
    "    \n",
    "    \n",
    "* 2. 문장에 대한 확률\n",
    "    P(An adorable little boy is spreading smiles)=\n",
    "P(An)×P(adorable|An)×P(little|An adorable)×P(boy|An adorable little)×P(is|An adorable little boy) ×P(spreading|An adorable little boy is)×P(smiles|An adorable little boy is spreading)\n",
    "\n",
    "\n",
    "* 3. 카운트 기반의 접근\n",
    "    - SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까? 정답은 카우트에 기반하여\n",
    "    - P(is|An adorable little boy)=count(An adorable little boy is)/count(An adorable little boy )\n",
    "    \n",
    "    \n",
    "* 4. 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
    "    * 언어 모델은 실생활에서 아용되는 언어의 확률 분포를 근사 모델링함\n",
    "    * 카운트 기반으로 접근하려고 한다면 corpus가 정말 방대한 양이 필요함\n",
    "        * if An adorable little boy is가 없으면 count(An adorable little boy is)가 0이 됨\n",
    "       \n",
    "    * 희소 문제(sparsity problem): 이와 같이 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제\n",
    "    * 문제 완화 방법: n-gram, 스무딩, 백오프 등의 일반화(generalization) 기법 존재\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-person",
   "metadata": {},
   "source": [
    "## 3) N-gram 언어 모델\n",
    "* 여전히 카운트 기반 통계적 접근\n",
    "* 하지만 모든 단어 고려가 아닌 일부 단어만 고려하는 접근 방법\n",
    "* 단어 몇 개 보느냐 --> n-gram에서 n의 의미\n",
    "\n",
    "### 1. 코퍼스에서 카운트하지 못하는 경우의 감소\n",
    "* SLM의 한계: 카운트할 수 없을 가능성이 높음\n",
    "* 앞 단어 중 임의의 개수만 포함해서 카운트ㅏ여 근사하자는 것. 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아짐.\n",
    "\n",
    "### 2. N-gram\n",
    "* 임의의 개수를 정하기 위한 기준을 위해 사용하는 것이 n-gram\n",
    "* n-gram은 n개의 연속적인 단어 나열 의미\n",
    "* 갖고 있는 코퍼스에서 n개 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주함\n",
    "    * ex) An adorable little boy is spreading smiles\n",
    "    * trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
    "    * 만약 4-gram이고 spreading 다음 단어 예측: boy is spreading까지 n-1개 단어 고려\n",
    "\n",
    "### 3. N-gram Langauge Model의 한계\n",
    "* n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생김\n",
    "* 전체 문장 고려한 언어 모델보다 정확도가 떨어짐\n",
    "\n",
    "\n",
    "* (1) 희소 문제(Sparsity Problem): 여전히 존재\n",
    "* (2) n을 선택하는 것은 trade-off 문제\n",
    "    - n이 크면 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제 점점 심각\n",
    "    - n이 커질수록 모델 사이즈가 커진다는 문제점\n",
    "    - 기본적으로 모든 n-gram에 대해서 카운트를 해야하기 때문\n",
    "    - 작게 선택 -> 훈련 코퍼스에서는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어짐\n",
    "    - max 5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-sunday",
   "metadata": {},
   "source": [
    "## 4) 한국어에서의 언어 모델(Langauge Model for Korean Sentences)\n",
    "    영어나 기타 언어에 비해서 한국어는 언어 모델로 다음 단어를 예측하기가 훨씬 까다로움\n",
    "\n",
    "\n",
    "### 1) 한국어는 어순이 중요하지 않다.\n",
    "### 2) 한국어는 교착어이다. \n",
    "    예를 들어 조사가 있음. 그렇기 때문에 토큰화를 통해 접사나 조사 등을 분리하는 것은 중요한 작업이 되기도 함.\n",
    "### 3) 한국어는 띄어쓰기가 제대로 지켜지지 않는다.\n",
    "    토큰이 제대로 분리되지 않은 채 훈련데이터로 사용한다면 언어 모델은 제대로 동작하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-footwear",
   "metadata": {},
   "source": [
    "## 5) Perplexity\n",
    "* 두 개의 모델 A, B 성능 비교\n",
    "* 두 개의 모델을 오타 교정, 기계 번역 등의 평가에 투입하여 성능을 비교\n",
    "* 외부 평가 대신 모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 내부 평가에 해당되는 perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-spanish",
   "metadata": {},
   "source": [
    "### 1. 언어 모델의 평가 방법(Evaluation metric): PPL\n",
    "    * PPL: 헷갈리는 정도. 수치가 낮을 수록 성능이 좋음\n",
    "    * 단어의 수로 정규화된 테스트 데이터에 대한 확률의 역수\n",
    "    * PPL을 최소화: 문장 확률 최대화\n",
    "\n",
    "### 2. 분기 계수(Branching factor)\n",
    "    * PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수(branching factor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-product",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
