{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "present-concrete",
   "metadata": {},
   "source": [
    "# 딥러닝을 이용한 자연어 처리 입문\n",
    "## 02. 텍스트 전처리(Text preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-chancellor",
   "metadata": {},
   "source": [
    "## 01) 토큰화(Tokenization)\n",
    "* 정의: 주어진 코퍼스에서 토큰이라 불리는 단위로 나누는 작업\n",
    "\n",
    "### 단어 토큰화(Word Tokenization)\n",
    "* 영어는 띄어쓰기로 구분 되지만 한국어는 그렇지 않음\n",
    "* NLTK는 영어 코퍼스를 토큰화하기 위한 도구들을 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "super-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Dont't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Dont't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "\n",
    "# Don't를 do와 n't로 분리, Jone's는 John과 's로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "genuine-armor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "\n",
    "# WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있음\n",
    "# 앞서 확인했던 word_tokenize와는 달리 Don't를 Don과 '와 t로 분리\n",
    "# Jone's를 Jone과 '와 s로 분리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "human-vertical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# 케라스토 토큰화 도구로서 text_to_word_sequence 제공\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "\n",
    "# 기본적으로 모든 알파벳을 소문자로 바꾸면서 마침표, 컴마, 느낌표 등 구두점을 제거\n",
    "# don't나 jone's 같은 경우 아포스트로피는 보존함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-north",
   "metadata": {},
   "source": [
    "토큰화에서 고려해야 할 사항\n",
    "* 1) 구두점이나 특수 문자를 단순 제외해서는 안 된다.\n",
    "* 2) 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "* 3) 표준 토큰화 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "realistic-portuguese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "# Penn Treebank Tokenization\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))\n",
    "\n",
    "# 하이푼은 하나로, 아포스트로피로 접어가 함께하면 분리\n",
    "# home-based는 하나로, doesn't는 does와 n't로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-compatibility",
   "metadata": {},
   "source": [
    "### 문장 토큰화(Sentence Tokenization)\n",
    "aka 문장 분류(Sentence segmentation)\n",
    "* NLTK에서는 영어 문장의 토큰화를 수행하는 sent_tokenize를 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "final-martial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 중간에 마침표가 여러 번 등장하는 경우\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "# 단순히 마침표를 구분자로 하여 문장을 구분하지 않았기 때문에 Ph.D.를 문장 내의 단어로 인식하여 성공적으로 인시가는 것을 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "given-zoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Using cached kss-2.5.0-py3-none-any.whl (68 kB)\n",
      "Installing collected packages: kss\n",
      "Successfully installed kss-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "legitimate-group",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "# 한국어에 대한 문장 토큰화 도구는 KSS(Korean Sentence Splitter)\n",
    "import kss\n",
    "\n",
    "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-guatemala",
   "metadata": {},
   "source": [
    "### 이진 분류기(Binary Classifier)\n",
    "* 문장 토큰화에서의 예외 사항을 발생시키는 마침표의 처리를 위해서 입력에 따라 두 개의 클래스로 분류하는 이진 분류기를 사용하기도 함\n",
    "* 두 개의 클래스: \n",
    "    마침표가 단어의 일부분일 경우, 즉 마침표가 약어로 쓰이는 경우\n",
    "    마침표가 정말로 문장의 구분자일 경우\n",
    "* 이를 결정하기 위해서는 어떤 마침표가 주로 약어에 사용되는지 알아야함. 약어 사전 활용.\n",
    "* 이러한 문장 토큰화 수행하는 오픈 소스: NLTK, OpenNLP, 스탠포드 CoreNLP, splitta, LingPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-scroll",
   "metadata": {},
   "source": [
    "* 한국어는 어절 토큰화(띄어쓰기 단위의 토큰화)는 지양되고 있음\n",
    "* 한국어는 교착어(조사, 어미 등을 붙여서 말을 만드는 언어)\n",
    "\n",
    "#### 1) 영어와 달리 한국어에는 조사라는 것이 존재\n",
    "- 한국어에서는 형태소(morpheme)이라는 개념을 반드시 이해\n",
    "- 형태소: 뜻을 가진 가장 작은 말의 단위\n",
    "    자립 형태소(접사, 어미 조사와 상관없이 사용. 자체로 단어. 체언, 수식언, 감탄사)와 의존 형태소(다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간)\n",
    "    \n",
    "- 영어에서의 단어 토큰화와 유사항 현태를 얻으려면 어절 토큰화가 아닌 형태소 토큰화를 해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-integer",
   "metadata": {},
   "source": [
    "#### 2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않음\n",
    "\n",
    "### 품사 태깅(Part of speech tagging)\n",
    "* ex. fly: 동사) 날다, 명사) 파리\n",
    "* ex. 못: 명사) 망치를 사용해 목재 고정, 부사) +먹는다, +달린다와 같은 동작 동사를 할 수 없음\n",
    "\n",
    "    \n",
    "* 결국 단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 보는 것이 주요 지표가 될 수 있음\n",
    "* 단어 토큰화 과정에서 품사 구분 <- 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "altered-acting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK에서는 PennTreebank POS Tags라는 기준 사용\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sublime-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('Ph.D.', 'NNP'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "x=word_tokenize(text)\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-superintendent",
   "metadata": {},
   "source": [
    "PRP(인칭대명사), VBP(동사), RB(부사), VBG(현재부사), IN(전치사), NNP(고유명사), NNS(복수형 명사), CC(접속사), DT(관사)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-portfolio",
   "metadata": {},
   "source": [
    "#### 한국어 자연어 처리를 위해서는 KoNLPy(코엔엘파이)라는 파이썬 패키지 사용\n",
    "* 코엔엘파이에서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)\n",
    "* 한국어 NLP에서의 형태소 분석기는 단어 토큰화라기보다는 정확히는 **형태소(morpheme) 단위로 형태소 토큰화(morpheme tokenization)**을 수행하게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "concrete-matthew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "\n",
    "# 형태소 추출(조사를 기본적으로 분석)\n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sunset-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "# 품사 태깅(POS tagging)\n",
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "insured-parks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "# 명사 추출\n",
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "appropriate-tenant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "# 형태소 추출\n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "italian-republican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "# 품사 태깅(POS tagging)\n",
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "collaborative-fitting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "# 명사 추출\n",
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-nature",
   "metadata": {},
   "source": [
    "## 02) 정제와 정규화(Cleaning and Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-degree",
   "metadata": {},
   "source": [
    "* 토큰화 작업 전, 후에는 텍스트를 용도에 맞게 정제 및 정규화함\n",
    "* 정제제: 갖고 있는 코퍼스로부터 노이즈 데이터 제거\n",
    "* 정규화: 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-pleasure",
   "metadata": {},
   "source": [
    "#### 1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
    "* ex. USA와 US, uh-huh와 uhhuh\n",
    "* 어간 추출(stemming)과 표제어 추출(lemmatization)\n",
    "\n",
    "#### 2. 대소문자 통합\n",
    "* 검색엔진 등에서\n",
    "* 일종의 규칙 등을 세움. 문장의 맨 앞에서 나오는 단어의 대문자만 소문자로 바꾸고, 다른 단어들은 전부 대문자인 상태로 둔다\n",
    "\n",
    "#### 3. 불필요한 단어의 제거(Removing unnecessary words)\n",
    "* 노이즈 데이터: 자연어가 아니면서 아무 의미도 갖지 않는 글자들(특수 문자) 혹은 분석하고자 하는 목적에 맞지 않는 불필요 단어들\n",
    "* 제거 방법으로는 불용어 제거, 등장 빈도가 적은 단어, 길이가 짧은 단어들을 제거하는 방법(esp 영어권 언어)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-municipality",
   "metadata": {},
   "source": [
    "__길이가 짧은 단어 제거 방법__\n",
    "* 한국어는 단어 평균 길이가 2-3으로 6-7인 영어보다 길어 적절하지 않을 수 있음\n",
    "* 한국어는 한 글자만으로도 이미 의미를 가진 경우가 많음\n",
    "\n",
    "#### 4. 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "binding-rover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1-2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-chapel",
   "metadata": {},
   "source": [
    "## 03) 어간 추출(Stemming)과 표제어 추출(Lemmatization)\n",
    "* 눈으로 봤을 때는 서로 다른 단어들이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화시켜서 문서 내의 단어수를 줄이겠다는 것\n",
    "* 이러한 방법들은 단어의 빈도수를 기반으로 문제를 풀고자 하는 BoW(Bag of Words) 표현을 사용하는 자연어처리 문제에서 주로 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-fluid",
   "metadata": {},
   "source": [
    "### 1. 표제어 추출(Lemmatization)\n",
    "* Lemma: 표제어, 기본 사전형 단어\n",
    "* Lemmatization: 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단\n",
    "    ex. am, are, is -> be\n",
    "* 가장 섬세한 방법: 단더의 형태학(morphology, 형태소로부터 단어들을 만들어가는 학문)적 파싱을 먼저 진행\n",
    "    \n",
    "* 형태소는 두 가지 종류: 어간(stem, 단어의 의미 담고 있는 핵심 부분)과 접사(affix, 단어의 추가적인 의미)\n",
    "* 형태학적 파싱은 이 두가지 구성 요소를 분리하는 작업 ex. cats: cat, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dying-russia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "# NLTK에서는 표제어 추출을 위한 도구인 WordNetLemmatizer를 지원\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "n = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])\n",
    "\n",
    "# Lemmatization은 Stemming과는 달리 단어의 형태가 적절히 보존되는 양상을 보임\n",
    "# WordNetLemmatizer는 입력으로 단어가 동사 품사라는 사실을 알려줄 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "assured-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "thorough-quantum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dated-beads",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-tiger",
   "metadata": {},
   "source": [
    "* Lemmatization은 문맥을 고려하여, 수행했을 때의 결과는 해당 언어의 품사 정보 보존(POS 태그 보존)\n",
    "* Stemming을 수행한 결과는 품사 정보가 보존되지 않음(POS 태그 고려 안 함). 사전에 존재하지 않는 단어일 경우 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-respect",
   "metadata": {},
   "source": [
    "### 2. 어간 추출(Stemming)\n",
    "* 어간을 추출하는 작업\n",
    "* 섬세한 작업이 아니므로 결과 단어는 사전에 존재하지 않을 수도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "funky-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "# 포터 알고리즘으로 수행\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer()\n",
    "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "czech-optimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "mysterious-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words=['formalize', 'allowance', 'electricical']\n",
    "print([s.stem(w) for w in words])\n",
    "\n",
    "# 포터 어간 추출기는 정밀하게 설계되어 정확도가 높으므로 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "unnecessary-tunisia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "s=PorterStemmer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "searching-melbourne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "l = LancasterStemmer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([l.stem(w) for w in words])\n",
    "\n",
    "# 전혀 다른 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-modem",
   "metadata": {},
   "source": [
    "### 3.한국어에서의 어간 추출\n",
    "5언 9품사의 구조\n",
    "* 체언: 명사, 대명사 수사\n",
    "* 수식언: 관형사, 부사\n",
    "* 관계언: 조사\n",
    "* 독립언: 감탄사\n",
    "* 용언: 동사, 형용사 (어간과 어미의 결합으로 구성)\n",
    "    \n",
    "(1) 활용(conjunction)\n",
    "* 한국어, 인도유럽어의 특징\n",
    "* 활용: 용언의 어간이 어미를 가지는 일\n",
    "- 어간(stem): 용언을 활용할 때 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분\n",
    "- 어미(ending): 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분. 여러 문법적 기능 수행.\n",
    "    \n",
    "(2) 규칙 활용\n",
    "* 어간이 어미를 취할 때, 어간의 모습이 일정\n",
    "* ex. 잡/어간 + 다/어미. 단순 부리해주면 어간 추출이 됨\n",
    "\n",
    "(3) 불규칙 활용\n",
    "* 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미\n",
    "* ex. ‘듣-, 돕-, 곱-, 잇-, 오르-, 노랗-’ 등이 ‘듣/들-, 돕/도우-, 곱/고우-, 잇/이-, 올/올-, 노랗/노라-’와 같이 어간의 형식이 달라지는 일이 있거나 ‘오르+ 아/어→올라, 하+아/어→하여, 이르+아/어→이르러, 푸르+아/어→푸르러’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-mention",
   "metadata": {},
   "source": [
    "## 04) 불용어(Stopword)\n",
    "* 유의미한 단어 토큰만 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업 필요\n",
    "* 큰 의미 없다: 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어\n",
    "    ex. I, my, me, over, 조사, 접미사 같은 단어들. 문장에서는 자주 등장하지만 실제 의미 분석에는 거의 기여하는 바가 없음\n",
    "* 이러한 단어들을 불용어(Stopword)라고 하고 NLTK에서는 100여개 이상 단어들을 패키지에서 미리 정의\n",
    "\n",
    "### 1. NLTK에서 불용어 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "painful-administration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]\n",
    "\n",
    "# NLTK가 정의한 영어 불용어 리스트 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-ranch",
   "metadata": {},
   "source": [
    "### 2. NLTK 통해서 불용어 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adverse-addition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-wright",
   "metadata": {},
   "source": [
    "### 3. 한국어에서 불용어 제거하기\n",
    "* 간단하게는 토큰화 후에 조사, 접속사 등을 제거\n",
    "* 하지만 조사 접속사 말고도 명사 형용사 중에서 불용어로서 제거하고 싶은 것들 있을 수 있음\n",
    "* 사용자가 직접 불용어 사전 만들게 되는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "meaning-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "stop_words = stop_words.split(' ')\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "# result=[word for word in word_tokens if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bacterial-receiver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-footage",
   "metadata": {},
   "source": [
    "## 05) 정규 표현식(Regular expression)\n",
    "* 정규표현식 모듈 re 사용 방법\n",
    "* NLTK를 통한 정규 표현식을 이용한 토큰화\n",
    "\n",
    "### 1. 정규 표현식 문법과 모듈 함수\n",
    "#### 1) 정규 표현식 문법\n",
    "* . : 한 개의 임의의 문자\n",
    "* ? :  앞의 문자가 존재할 수도, 않을 수도(0 or 1개)\n",
    "* * : 앞의 문자가 무한개로 존재할 수도, 존재하지 않을 수도(0개 이상)\n",
    "* + : 앞의 문자가 최소 한 개 이상 존재(문자가 1개 이상)\n",
    "* ^ : 뒤의 문자로 문자열이 시작됨\n",
    "* $ : 앞의 문자로 문자열이 끝남\n",
    "* {숫자} : 숫자만큼 반복\n",
    "* {숫자1, 숫자2}: 숫자1이상 숫자2 이하만큼 반복함. ?, *, +를 이것으로 대체할 수 있음\n",
    "* {숫자,}: 숫자 이상만큼 반복\n",
    "* []: 대괄호 안의 문자들 중 한 개의 문자와 매치함. [amk]라 한다면 a 또는 m 또는 k 중 하나라도 존재하면 매치를 의미\n",
    "    [a-z]와 같이 범위를 지정, [a-zA-Z]는 알파벳 전체를 의미하는 범위, 문자열에 알파벳이 존재하면 매치 의미\n",
    "* [^]: 해당 문자를 제외한 문자를 매치\n",
    "* | : A|B와 같이 쓰이며 A 또는 B의 의미를 가짐\n",
    "    \n",
    "__역슬래쉬(\\)를 이용하여 자주 쓰이는 문자 규칙들__\n",
    "* \\\\: 역슬래쉬 문자 자체를 의미\n",
    "* \\d: 모든 숫자 의미 [0-9]\n",
    "* \\D: 숫자 제외한 모든 문자[^0-9]\n",
    "* \\s: 공백을 의미 [\\t\\n\\r\\f\\v]와 의미가 동일\n",
    "* \\S: 공백을 제외한 문자 의미 [^ \\t\\n\\r\\f\\v]\n",
    "* \\w: 문자나 숫자 의미 [a-zA-Z0-9]\n",
    "* \\W : 문자나 숫자가 아닌 문자 의미 [^a-zA-Z0-9]\n",
    "    \n",
    "__정규 표현식 모듈 함수__\n",
    "* re.compile(): 정규표현식을 파이썬에게 전달\n",
    "* re.search(): 문자열 전체에 대해 정규표현식과 매치되는지 검색\n",
    "* re.match(): 문자열의 처음이 정규표현식과 매치되는지 검색\n",
    "* re.split(): 정규표현식 기준으로 문자열을 분리하여 리스트로 리턴\n",
    "* re.findall(): 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴\n",
    "    매치되는 문자열이 없다면 빈 리스트 리턴\n",
    "* re.finditer(): 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체 리턴\n",
    "* re.sub(): 문자열에서 정규 표현식과 매치되는 부분에 대해서 다른 문자열로 대체"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-guidance",
   "metadata": {},
   "source": [
    "### 3. 정규 표현식 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "rural-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "r=re.compile(\"a.c\")\n",
    "r.search(\"kkk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "physical-florida",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "twelve-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? 앞의 문자가 존재할 수도, 안 할 수도\n",
    "r=re.compile(\"ab?c\")\n",
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "clear-jungle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "catholic-theme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "innovative-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * 앞의 문자가 0개 이상\n",
    "r=re.compile('ab*c')\n",
    "r.search(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "leading-invite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "peripheral-assistant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efficient-background",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "norman-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +는 앞의 문자가 최소 1개 이상\n",
    "r=re.compile(\"ab+c\")\n",
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "magnetic-catering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "responsible-portfolio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 8), match='abbbbbbc'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "english-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^는 시작되는 글자 지정\n",
    "import re\n",
    "r=re.compile(\"^a\")\n",
    "r.search(\"bbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "partial-drawing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "essential-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 문자를 숫자만큼 반복\n",
    "r=re.compile(\"ab{2}c\")\n",
    "r.search(\"ac\")\n",
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "premier-television",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "chinese-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "r=re.compile(\"ab{2,8}c\")\n",
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cognitive-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "absent-homework",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "played-currency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 10), match='abbbbbbbbc'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "exposed-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search(\"abbbbbbbbbbbbbbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "handy-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 문자를 숫자 이상 만큼 반복\n",
    "r=re.compile(\"a{2,}bc\")\n",
    "r.search(\"bc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "immune-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search(\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "magnetic-market",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='aabc'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "architectural-villa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 18), match='aaaaaaaaaaaaaaaabc'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aaaaaaaaaaaaaaaabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "focused-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# []는 그 문자들 중 한 개의 문자와 매치라는 의미\n",
    "r=re.compile(\"[abc]\") \n",
    "r.search(\"zzz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "apart-catering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "assumed-executive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aaaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "elder-albany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='b'>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"baaaac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "independent-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알파벳 소문자에 대해서만 범위 지정\n",
    "r=re.compile(\"[a-z]\")\n",
    "r.search(\"AAA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "physical-stability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "legal-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search(\"111\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bronze-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ 기호 뒤 문자들을 제외한 모든 문자를 매치하는 역할\n",
    "r=re.compile(\"[^abc]\")\n",
    "r.search(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "african-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search(\"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "union-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search(\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "everyday-beach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "isolated-integer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='1'>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-round",
   "metadata": {},
   "source": [
    "### 3. 정규 표현식 모듈 함수 예제 \n",
    "(1) re.match()와 re.search()의 차이\n",
    "* search()가 정규 표현식 전체에 대해서 문자열이 매치하는지를 본다면, match()는 문자열의 첫 부분부터 정규표현식과 매치하는지를 확인함\n",
    "* 문자열 중간에 찾을 패턴이 있다고 하더라도, match함수는 시작부터 일치하지 않으면 찾지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "lined-supplier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 6), match='abc'>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = re.compile(\"ab.\")\n",
    "r.search(\"kkkabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "arranged-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.match(\"kkkabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "subject-partnership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.match(\"abckkk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-stake",
   "metadata": {},
   "source": [
    "(2) re.split()\n",
    "* 입력된 정규 표현식 기준으로 문자열들을 분리하여 리스트로 리턴\n",
    "* 토큰화에 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "electrical-filename",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "contemporary-death",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"\"\"사과\n",
    "딸기\n",
    "수박\n",
    "메론\n",
    "바나나\"\"\"\n",
    "re.split(\"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "offshore-corpus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"사과+딸기+수박+메론+바나나\"\n",
    "re.split(\"\\+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-characteristic",
   "metadata": {},
   "source": [
    "(3) re.findall()\n",
    "* 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴\n",
    "* 매치X -> 빈 문자열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "classical-potter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"\"\"이름 : 김철수\n",
    "전화번호 : 010 - 1234 - 1234\n",
    "나이 : 30\n",
    "성별 : 남\"\"\"  \n",
    "re.findall(\"\\d+\", text) # 전체 텍스트로부터 숫자만 찾아내서 리스트로 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "acquired-antenna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\d+\", \"문자열입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-danger",
   "metadata": {},
   "source": [
    "(4) re.sub()\n",
    "* 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bright-secondary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "re.sub('[^a-zA-Z]', ' ', text)\n",
    "# 특수 문자 제거(알파벳 외의 문자는 공백으로 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "critical-champion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 정규 표현식 텍스트 전처리 예제\n",
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\" \n",
    "re.split(\"\\s+\", text) # 최소 1개 이상의 공백인 패턴 찾아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "compact-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '101', '102']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('\\d+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "multiple-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대문자인 행의 값\n",
    "re.findall('[A-Z]', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "lesbian-eleven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]{4}', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "appropriate-overhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]+', text) #이름은 대문자와 소문자가 섞여있음. 처음에 대문자 등장하고 뒤에 소문자 여러번 등장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "unnecessary-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_only = re.sub('[^a-zA-Z]', ' ', text) # 영문자 아니면 전부 공백으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "legitimate-senior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    John    PROF     James   STUD     Mac   STUD'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters_only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-helicopter",
   "metadata": {},
   "source": [
    "### 6. 정규 표현식을 이용한 토큰화\n",
    "* NLTK에서는 정규 표현식을 사용해서 단어 토큰화를 수행하는 RegexpTokenizer를 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "active-religious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\") # \\+는 문자나 숫자가 1개 이상인 경우 인식. 구두점 제외하고 단어만\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "informal-oasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# 괄호 안에 정규 표현식 혹은 토큰 나누기 위한 기준 입력\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\s]+\", gaps=True) #gaps=True는 해당 정규 표현식을 토큰ㅇ르ㅗ 나누기 위한 기준으로 사용한다는 의미\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-maryland",
   "metadata": {},
   "source": [
    "## 06) 정수 인코딩(Integer Encoding)\n",
    "* 각 단어를 고유한 정수에 매핑시키는 전처리 작업\n",
    "* 단어에 대한 빈도수를 기준으로 정렬한 뒤에 부여함\n",
    "\n",
    "### 1. 정수 인코딩(Integer Encoding)\n",
    "* 단어를 빈도수 순으로 정렬한 단어 집합을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "complete-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) dictionary 사용하기\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "prospective-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "hungry-billy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화\n",
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "significant-shuttle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "# 정제와 단어 토큰화\n",
    "vocab = {}\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence = word_tokenize(i)\n",
    "    result = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        word = word.lower() # 모든 단어 소문자화하여 단어의 개수 줄임\n",
    "        if word not in stop_words:\n",
    "            if len(word) > 2:\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1\n",
    "    sentences.append(result)\n",
    "print(sentences)\n",
    "\n",
    "# 동일한 단어가 대문자로 표기되었다는 이유로 서로 다른 단어로 카운트되는 일이 없도록 모든 단어를 소문자로 바꾸어놓음\n",
    "# 자연어처리에서 큰 의미를 갖지 못하는 불용어와 길이가 짧은 단어를 제거하는 방법 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "conditional-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab) # 중복을 제거한 단어와 각 단어에 대한 빈도수가 기록되어 있음\n",
    "# 단어가 key, 단어에 대한 빈도수가 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "sharing-agenda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab['barber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ruled-prairie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 높은 순서대로 정렬\n",
    "vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "mechanical-office",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스 부여\n",
    "word_to_index = {}\n",
    "i=0\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    if frequency > 1:\n",
    "        i=i+1\n",
    "        word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "honey-soviet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "#상위 5개 단어만 사용\n",
    "vocab_size = 5\n",
    "#인덱스가 5 초과인 단어 제거\n",
    "words_frequency = [w for w, c in word_to_index.items() if c >= vocab_size + 1]\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w] #해당 단어에 대한 인덱스 정보를 삭제\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "conditional-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_to_index 사용하여 단어 토큰화가 된 상태로 저장된 sentences에 있는 각 단어를 정수로 바꾸는 작업\n",
    "# OOV: 단어 집합에 없는 단어\n",
    "word_to_index['OOV'] = len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "focused-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "# word_to_index를 사용하여 sentences의 모든 단어들을 매핑되는 정수로 인코딩\n",
    "encoded = []\n",
    "for s in sentences:\n",
    "    temp = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-armstrong",
   "metadata": {},
   "source": [
    "### 2) Counter 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "metropolitan-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "geological-adrian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# sentences는 단어 토큰화가 된 결과가 저장되어 있음\n",
    "# 단어 집합 vocabulary를 만들기 위해서는 sentences에서 문장의 경계인 [,]를 제거하고 단어들을 하나의 리스트로 만들기\n",
    "\n",
    "words = sum(sentences, [])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "spanish-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# 파이썬의 Counter()의 입력으로 사용하면 중복을 제거하고 단어의 빈도수 기록 가능\n",
    "vocab = Counter(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "disabled-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# 단어를 key, 단어에 대한 빈도수를 value로 저장\n",
    "print(vocab['barber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "amino-movement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "distinguished-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수일 수록 낮은 정수 인덱스\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab:\n",
    "    i = i+1\n",
    "    word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-practitioner",
   "metadata": {},
   "source": [
    "### 3) NLTK의 FreqDist 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fuzzy-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "greenhouse-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.hstack으로 문장 구분을 제거하여 입력으로 사용. ex) ['barber', 'person', 'barber', 'good']\n",
    "vocab = FreqDist(np.hstack(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cathedral-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#ㅠㅁ귣ㄱfksms eksdjdml qlsehtn cnffur\n",
    "print(vocab['barber'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "tight-mayor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "olympic-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# enumerate 사용\n",
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "polar-jumping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x1f241d3cb88>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-alias",
   "metadata": {},
   "source": [
    "### 4) enumerate 이해하기\n",
    "* 순서가 있는 자료형(list set tuple dictionary string)을 입력으로 받아 인텍스를 순차적으로 함께 리턴한다는 특징 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "little-general",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index: 0\n",
      "value : b, index: 1\n",
      "value : c, index: 2\n",
      "value : d, index: 3\n",
      "value : e, index: 4\n"
     ]
    }
   ],
   "source": [
    "test = ['a', 'b', 'c', 'd', 'e']\n",
    "for index, value in enumerate(test):\n",
    "    print(\"value : {}, index: {}\".format(value, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-guarantee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
